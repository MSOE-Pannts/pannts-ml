{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras as ks\n",
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, confusion_matrix\n",
    "from scripts.functions import create_model\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variables to be used in Keras and the CNN\n",
    "\n",
    "# number of items to use for training\n",
    "BATCH_SIZE = 100 \n",
    "\n",
    "# Number of identifying classes \n",
    "#   WE have two, Bloom and no bloom 1/0\n",
    "NUM_CLASSES = 2 \n",
    "\n",
    "# number of times to repeat process\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('../../data/cleaned/site1_vineyard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['BGA-Phycocyanin RFU'].apply(lambda x : x/0.2334)\n",
    "target\n",
    "dataset = df.drop(columns=['Chlorophyll (ug/L)', 'Chlorophyll RFU'])\n",
    "dataset['BGA (ug/L)'] = target\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "timestamp = dataset['Date (mm.dd.yyyy)'] + ' '+ dataset['Time 24hr']\n",
    "timestamp = pd.to_datetime(timestamp)\n",
    "dataset['Timestamp'] = timestamp\n",
    "dataset.head()\n",
    "#converts the date object to a numerical representation of that object\n",
    "#dataset['date (mm.dd.yyyy)'] = (dataset['date (mm.dd.yyyy)'] - dataset['date (mm.dd.yyyy)'].min()) / np.timedelta64(1,'D')\n",
    "#dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dont need data and time now that we have Timestamp. Lets remove them\n",
    "\n",
    "dataset = dataset.drop(columns=['Date (mm.dd.yyyy)', 'Time 24hr'])\n",
    "dataset.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = dataset['BGA (ug/L)'].apply(lambda x: 1 if x > 20 else 0)\n",
    "dataset['Bloom'] = target\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try to normalize this now....\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "dataset_columns = ['Temp C','Sp Cond (uS/cm)', 'pH (mV)','pH', 'Turbidity (NTU)', 'ODOSat%','ODO (mg/L)', 'Bloom']\n",
    "scaler = MinMaxScaler()\n",
    "ds_scaled = scaler.fit_transform(dataset[dataset_columns])\n",
    "dataset = pd.DataFrame(ds_scaled,columns=dataset_columns)\n",
    "dataset.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to take a moving window of the data of 10 time stamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "determines the window size for the daata set\n",
    "@param dataset - The dataset to get windows for\n",
    "@param window_size - the size of the window  \n",
    "@param shift - the amout to shift the window\n",
    "'''\n",
    "def windows(dataset, window_size, shift):\n",
    "    start = 0\n",
    "    while start+window_size < dataset.shape[0]: \n",
    "        yield (int(start), int(start+window_size))\n",
    "        # shift the window five blocks of time\n",
    "        start += shift\n",
    "        if start % 300 == 0:\n",
    "            print('Window Segmentation {0:.2f}% done'.format(((start+window_size) / dataset.shape[0]) * 100 ))\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "Segments the dataset based on the parameters that are passed in.\n",
    "@param dataset - the dataset to segment into window\n",
    "@param columns - the array of columns from the dataset to be looked at\n",
    "@param window_size - the size of the window you would like to be looked at. Defualt is 10\n",
    "\n",
    "'''\n",
    "def segment_dataset(dataset, columns, target, window_size=10):    \n",
    "    print('WINDOW SIZE',window_size)\n",
    "    print('NUMBER OF COULUMNS',len(columns))\n",
    "    segments = np.empty((0, window_size, len(columns)))\n",
    "    labels = np.empty((0))\n",
    "    count = 0\n",
    "    for (start, end) in windows(dataset, window_size, 1):\n",
    "        count+=1\n",
    "        values = dataset[columns][start:end]\n",
    "        if(values.shape[0] == window_size):\n",
    "            segments = np.vstack([segments, np.stack([values])])\n",
    "            # Takes the larger of the two variables if there are more than one. \n",
    "            # This makes it more likly to predict a bloom. Can be changed to iloc[0] to\n",
    "            # be less likly to predict a bloom (more 0s in the label array)\n",
    "            \n",
    "            labels = np.append(labels, dataset[target][start:end].mode().iloc[-1])\n",
    "        else:\n",
    "            print(\"No more Windows available... Exiting\")\n",
    "            break\n",
    "    return (segments, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = dataset_columns[:-1]\n",
    "(segments, labels) = segment_dataset(dataset, feature_columns, 'Bloom', 9)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaping the data to be used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = segments.reshape(len(segments),9,7,1)\n",
    "segments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what the heck does this look like now?\n",
    "X = 5\n",
    "y = 40\n",
    "plt.figure(figsize=(X,y))\n",
    "columns = 10\n",
    "for x in range(0, 10):\n",
    "    plt.subplot(len(segments) / columns + 1, columns, x + 1)\n",
    "    plt.imshow(segments[x][0]*255,cmap='gray')\n",
    "#plt.imshow(segments[0][0] * 255, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.reshape(labels.shape[0],1)\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking apart training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(segments, labels, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train shape:\",x_train.shape)\n",
    "print(\"x_test shape:\",x_test.shape)\n",
    "print(\"y_train shape:\",y_train.shape)\n",
    "print(\"y_test shape:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mod = ks.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_mod = ks.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "input_shape = (9,7,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Come on, let's create the model already!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "from keras.layers import Dense, Dropout, Conv2D, Flatten, Activation, MaxPooling2D\n",
    "\n",
    "# Gets the precision of the different metrics\n",
    "def create_class_predictions(pred):\n",
    "    retval = np.array([])\n",
    "    for row in pred:\n",
    "        max_value = (-1,-1)\n",
    "        for index, value in enumerate(row):\n",
    "            if value > max_value[1]:\n",
    "                max_value = (index, value)\n",
    "        retval = np.append(retval, max_value[0])\n",
    "    return retval\n",
    "\n",
    "\n",
    "def create_layers(num_layers):\n",
    "    layers = [Flatten(), Dropout(0.2), Dense(NUM_CLASSES, activation='softmax', input_dim=2)]\n",
    "    for i in range(0, num_layers):\n",
    "        layers.insert(0, Conv2D(44, 7, input_shape=input_shape, activation='relu', padding='same'))\n",
    "    return layers\n",
    "    \n",
    "values = []\n",
    "\n",
    "print(\"...And Here we go....\")\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Started at\",st)\n",
    "max_layers = 10\n",
    "for i in range(2, max_layers+1):\n",
    "    layers = create_layers(i)\n",
    "    model = create_model(44, 7, input_shape, NUM_CLASSES, 0.0001, layers=layers)\n",
    "    model.fit(x=x_train, y=y_train_mod, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=0)\n",
    "    # What is our score?\n",
    "    score = model.evaluate(x_train, y_train_mod, verbose=0)\n",
    "    predictions = model.predict(x_test)\n",
    "    predict = create_class_predictions(predictions)\n",
    "    recall = recall_score(y_test.reshape(-1,), predict)\n",
    "    value = (i, recall)\n",
    "    values.append(value)\n",
    "    print(\"Layers:{}, Recall:{}\".format(i, recall))\n",
    "    print(\"Current Time: \", st)\n",
    "\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Finished at\",st)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets try iterating over neurons and and window size\n",
    "values = []\n",
    "\n",
    "\n",
    "print(\"...And Here we go....\")\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Started at\",st)\n",
    "min_neurons = 12\n",
    "max_neurons = 44\n",
    "for num_neurons in range(min_neurons, max_neurons+1):\n",
    "    for window_size in range(2, 10):\n",
    "        window = (window_size, window_size)\n",
    "        layers = [Conv2D(num_neurons, window, input_shape=input_shape, activation='relu', padding='same'),\n",
    "                  Conv2D(num_neurons * 2, window_size, activation='relu', padding='same'),\n",
    "                  Dense(num_neurons), Flatten(), Dropout(0.2), Dense(NUM_CLASSES, activation='softmax')]\n",
    "        model = create_model(num_neurons, window, input_shape, NUM_CLASSES, 0.1, layers=layers)\n",
    "        model.fit(x=x_train, y=y_train_mod, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=0)\n",
    "        # What is our score?\n",
    "        score = model.evaluate(x_train, y_train_mod, verbose=0)\n",
    "        predictions = model.predict(x_test)\n",
    "        predict = create_class_predictions(predictions)\n",
    "        recall = recall_score(y_test.reshape(-1,), predict)\n",
    "        value = (i, window_size, recall)\n",
    "        values.append(value)\n",
    "        print('Number of Neurons: {}\\nWindow Size: {}\\nRecall:{}'.format(num_neurons, window_size, recall))\n",
    "    print(values[-10])\n",
    "\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Finished at\",st)\n",
    "print(values)\n",
    "for value in values:\n",
    "    if value[2] > 0:\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for minimum values that give the best recall.\n",
    "values = []\n",
    "neurons = 20\n",
    "print(\"...And Here we go....\")\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Started at\",st)\n",
    "model = create_model(neurons, 7, input_shape, NUM_CLASSES, 0.0001)\n",
    "history = model.fit(x=x_train, y=y_train_mod, batch_size=BATCH_SIZE, epochs=100, verbose=1)\n",
    "# What is our score?\n",
    "score = model.evaluate(x_train, y_train_mod, verbose=1)\n",
    "predictions = model.predict(x_test)\n",
    "predict = create_class_predictions(predictions)\n",
    "recall = recall_score(y_test.reshape(-1,), predict)\n",
    "value = (neurons, recall)\n",
    "values.append(value)\n",
    "print(\"Layers:{}, Recall:{}\".format(neurons, recall))\n",
    "st = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(\"Finished at\",st)\n",
    "print(history.history.keys())\n",
    "sns.lineplot(data=pd.DataFrame({'loss': history.history['loss']}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignoring dropout for deployment\n",
    "K.set_learning_phase(0)\n",
    " \n",
    "# Set a file path to save the model in.\n",
    "model_name = \"cnn_model\"\n",
    "model_version = \"1\"\n",
    "tf_path = \"./../../saved_models/{}/{}\".format(model_name, model_version)\n",
    " \n",
    "# Get the session from the Keras back-end to save the model in TF format.\n",
    "with K.get_session() as sess:\n",
    "    tf.saved_model.simple_save(sess, tf_path, inputs={'input': model.input}, outputs={t.name: t for t in model.outputs})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
